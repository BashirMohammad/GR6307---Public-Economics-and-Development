#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass beamer
\begin_preamble
\usepackage{ifthen}
\usepackage{multirow,bigstrut}
\usepackage{tikz}
\usetikzlibrary{patterns,decorations.pathreplacing,shapes,snakes}
\usetikzlibrary{arrows}
\usepackage{rotating}
\usepackage{pdflscape}
\usepackage{makecell}
\usepackage{graphicx}
\usepackage{animate}
\usepackage{pifont}
\usepackage{booktabs}
\usepackage{relsize}
%\usepackage{adjustbox}
%\usepackage{txfonts}
%\usepackage{handoutWithNotes}
%\pgfpagesuselayout{3 on 1 with notes}[a4paper,border shrink=5mm]

% FOOTLINE - PAGE NUMBER RIGHT
\defbeamertemplate*{footline}{guildford foot theme}
{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.7\paperwidth,ht=1cm,dp=0ex,left]{}%
    {
    \insertsectionnavigationhorizontal{.5\paperwidth}{}{}
    }
 \end{beamercolorbox}
 \begin{beamercolorbox}[wd=0.31\paperwidth,ht=1cm,dp=0ex,right]{}%
{\tiny
\insertframenumber{} / \inserttotalframenumber\hspace*{5ex}
}
 \end{beamercolorbox}}%
  \vskip5pt%
}

\beamertemplatenavigationsymbolsempty
\usefonttheme{professionalfonts}
\usecolortheme[RGB={0,0,125}]{structure} 
\setbeamersize{ text margin left=10px}
%\definecolor{newblue}{rgb}{0,0,0.6}
%\setbeamercolor{alerted text}{fg=newblue}
\setbeamertemplate{frametitle}[default][center]

%{\bfseries\insertframetitle\par}

\RequirePackage{ifthen}

\newboolean{sectiontoc}
\setboolean{sectiontoc}{true} % default to true

\AtBeginSubsection[] 
{ 
  \ifthenelse{\boolean{sectiontoc}}{
  \begin{frame}[plain]
    \frametitle{Outline} 
    \tableofcontents[sectionstyle=show/hide,subsectionstyle=show/shaded/hide] 
  \end{frame} 
} 
}

\AtBeginSection[] 
{ 
  \ifthenelse{\boolean{sectiontoc}}{
  \begin{frame}[plain]
    \frametitle{Outline} 
    \tableofcontents[sectionstyle=show/shaded,subsectionstyle=show/hide/hide] 
  \end{frame} 
} 
}

\newcommand{\toclesssection}[1]{
   \setboolean{sectiontoc}{false}
   \section{#1}
   \setboolean{sectiontoc}{true}
}

\newcommand{\toclesssubsection}[1]{
   \setboolean{sectiontoc}{false}
   \subsection{#1}
   \setboolean{sectiontoc}{true}
}

\setbeameroption{hide notes}

\newcommand{\ShortNameSection}[2][]{
   \setboolean{sectiontoc}{false}
   \section[#1]{#2}
   \setboolean{sectiontoc}{true}
}

\newcommand{\light}[1]{\textcolor{gray}{#1}}
\end_preamble
\options notes=show
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package none
\inputencoding auto
\fontencoding T1
\font_roman "cmr" "default"
\font_sans "helvet" "default"
\font_typewriter "cmtt" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures false
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks false
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder true
\pdf_colorlinks true
\pdf_backref section
\pdf_pdfusetitle false
\pdf_quoted_options "linkcolor=blue, citecolor=blue, urlcolor=blue"
\papersize default
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 0
\use_package esint 1
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
setcounter{framenumber}{0}
\end_layout

\begin_layout Plain Layout


\backslash
thispagestyle{empty}
\end_layout

\end_inset

GR 6307
\begin_inset Newline newline
\end_inset

Public Economics and Development
\begin_inset Newline newline
\end_inset


\begin_inset Formula $\;$
\end_inset


\begin_inset Newline newline
\end_inset

4.
 Building State Capacity
\begin_inset Newline newline
\end_inset

with Data & Technology
\end_layout

\begin_layout Author
Michael Best
\end_layout

\begin_layout Date
Spring 2018
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Section
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Prediction in Government
\end_layout

\end_inset

Prediction Problems in Government
\end_layout

\begin_layout Subsection
Kleinberg, Ludwig, Mullainathan & Obermeyer (AER PnP 2015) 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
protect
\backslash
textit{
\end_layout

\end_inset

Prediction Policy Problems
\begin_inset ERT
status open

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kleinberg et al 2015: Prediction
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
In empirical policy research, we often focus on 
\emph on
causal inference
\emph default
.
 Policy choices depend on understanding the counterfactualâ€“what would happen
 without the policy.
\end_layout

\begin_layout Itemize
But, there are many policy problems where causal inference is not central,
 or even necessary!
\end_layout

\begin_layout Itemize
Consider 2 toy examples:
\end_layout

\begin_deeper
\begin_layout Enumerate
There is a drought.
 Should we invest in a rain dance to increase the chance of rain?
\end_layout

\begin_layout Enumerate
It is cloudy.
 Should I take an umbrella to work?
\end_layout

\end_deeper
\begin_layout Itemize
In both cases data are going to be useful.
 But they require different estimators:
\end_layout

\begin_deeper
\begin_layout Enumerate
Do rain dances cause rain? 
\emph on
Causal inference
\end_layout

\begin_layout Enumerate
Is the chance of rain high enough to merit an umbrella? 
\emph on
Prediction inference
\end_layout

\end_deeper
\begin_layout Itemize
This paper: Policy prediction problems are everywhere, and machine learning
 can help us solve them more effectively.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kleinberg et al 2015: Prediction and Causation
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Consider this framework for thinking about this.
\end_layout

\begin_layout Itemize
Let 
\begin_inset Formula $Y$
\end_inset

 be an outcome (rain) that depends on 
\begin_inset Formula $X_{0}$
\end_inset

 (a policy choice) and other 
\begin_inset Formula $X$
\end_inset

s.
\end_layout

\begin_layout Itemize
The policymaker must pick 
\begin_inset Formula $X_{0}$
\end_inset

 (umbrella, rain dance) to maximize known payoff 
\begin_inset Formula $\pi\left(X_{0},Y\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Decision depends on 
\begin_inset Formula 
\[
\frac{d\pi\left(X_{0},Y\right)}{dX_{0}}=\frac{\partial}{\partial X_{0}}\underbrace{\pi\left(Y\right)}_{\text{prediction}}+\frac{\partial\pi}{\partial Y}\underbrace{\frac{\partial Y}{\partial X_{0}}}_{\text{causation}}
\]

\end_inset


\end_layout

\begin_layout Enumerate
Prediction: We know the payoff, but we need to evaluate it at 
\begin_inset Formula $Y$
\end_inset

, so we need to predict what 
\begin_inset Formula $Y$
\end_inset

 will be
\end_layout

\begin_layout Enumerate
Causality: How much will 
\begin_inset Formula $Y$
\end_inset

 change if I change 
\begin_inset Formula $X_{0}$
\end_inset

?
\end_layout

\begin_layout Itemize
2 things to note
\end_layout

\begin_deeper
\begin_layout Enumerate
Prediction is useful when 
\begin_inset Formula $\partial\pi/\partial X$
\end_inset

 depends on 
\begin_inset Formula $Y$
\end_inset

 (benefit of umbrella depends on rain)
\end_layout

\begin_layout Enumerate
Only 
\begin_inset Formula $\hat{Y}$
\end_inset

 enters the decision, so we just need a low error estimate of 
\begin_inset Formula $\hat{Y}$
\end_inset

, not an unbiased or causal one
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kleinberg et al 2015: Machine Learning
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Standard empirical techniques aren't great for prediction because they focus
 on 
\series bold
unbiasedness
\series default
.
\end_layout

\begin_layout Itemize
e.g.
 Suppose you have 2 variables to predict 
\begin_inset Formula $y$
\end_inset

 and you get OLS estimates 
\begin_inset Formula $\hat{\beta}_{1}=1\pm0.001$
\end_inset

 and 
\begin_inset Formula $\hat{\beta}_{2}=4\pm10$
\end_inset

.
 What's the best prediction? 
\begin_inset Formula $x_{1}+4x_{2}$
\end_inset

? or perhaps the unbiased estimator 
\begin_inset Formula $x_{1}$
\end_inset

 since 
\begin_inset Formula $\hat{\beta}_{2}$
\end_inset

 is noisy?
\end_layout

\begin_layout Itemize
General setup: 
\end_layout

\begin_deeper
\begin_layout Itemize
Suppose you have a dataset 
\begin_inset Formula $D$
\end_inset

 of 
\begin_inset Formula $n$
\end_inset

 points 
\begin_inset Formula $\left(y_{i},x_{i}\right)\sim G$
\end_inset


\end_layout

\begin_layout Itemize
Use this data to pick a function 
\begin_inset Formula $\hat{f}\in\mathcal{F}$
\end_inset

 to predict the 
\begin_inset Formula $y$
\end_inset

 value of a new data point 
\begin_inset Formula $\left(y,x\right)\sim G$
\end_inset


\end_layout

\begin_layout Itemize
Goal is to minimize a loss function 
\begin_inset Formula $\mathcal{L}\left(y,\hat{f}\right)=\left(y-\hat{f}\left(x\right)\right)^{2}$
\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kleinberg et al 2015: Machine Learning
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
OLS minimizes 
\emph on
in-sample 
\emph default
error by choosing among linear functions 
\begin_inset Formula $\mathcal{F}_{lin}$
\end_inset


\begin_inset Formula 
\[
\hat{f}_{OLS}=\underset{f_{\beta}\in\mathcal{F}_{lin}}{\arg\min}\sum_{i=1}^{n}\left(y_{i}-f\left(x_{i}\right)\right)^{2}
\]

\end_inset


\end_layout

\begin_layout Itemize
This does great in-sample (sum over the 
\begin_inset Formula $i$
\end_inset

s in 
\emph on
this
\emph default
 dataset).
 But could do 
\emph on
arbitrarily
\emph default
 
\emph on
badly
\emph default
 on a new dataset (out of sample)!!
\begin_inset Formula 
\begin{align*}
MSE\left(x\right) & \equiv\mathbb{E}_{D}\left[\left(\hat{f}\left(x\right)-y\right)^{2}\right]\\
 & =\underbrace{\mathbb{E}_{D}\left[\left(\hat{f}\left(x\right)-\mathbb{E}_{D}\left[\hat{y}_{0}\right]\right)^{2}\right]}_{\text{Variance}}+\underbrace{\left(\mathbb{E}_{D}\left[\hat{y}_{0}\right]-y\right)^{2}}_{\text{Bias}^{2}}
\end{align*}

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kleinberg et al 2015: Machine Learning
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
So what's the alternative? ML techniques minimize
\begin_inset Formula 
\[
\hat{f}_{ML}=\underset{f\in\mathcal{F}}{\arg\min}\sum_{i=1}^{n}\left(y_{i}-f\left(x_{i}\right)\right)^{2}+\lambda R\left(f\right)
\]

\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $R\left(f\right)$
\end_inset

 is a 
\emph on
regularizer
\emph default
 that penalizes choosing functions that create variance.
 Constructed so that the set of functions 
\begin_inset Formula $\mathcal{F}_{c}=\left\{ f\vert R\left(f\right)\leq c\right\} $
\end_inset

 creates more variable predictions as 
\begin_inset Formula $c$
\end_inset

 increases.
\end_layout

\begin_layout Itemize
For linear models, larger coefficients generate more variable predictions
 so a natural regularizer is 
\begin_inset Formula $R\left(f_{\beta}\right)=\left\Vert \beta\right\Vert ^{d}$
\end_inset

 (
\begin_inset Formula $d=1$
\end_inset

=LASSO, 
\begin_inset Formula $d=2$
\end_inset

=ridge)
\end_layout

\begin_layout Itemize
\begin_inset Formula $\lambda$
\end_inset

 is the price at which we trade off variance and bias.
 OLS has infinite price for bias 
\begin_inset Formula $1/\lambda=\infty$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kleinberg et al 2015: Machine Learning
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
How should we pick 
\begin_inset Formula $\lambda$
\end_inset

? A key insight of machine learning is that you can ask the data to tell
 you:
\end_layout

\begin_layout Itemize
Split the data into 
\begin_inset Formula $f$
\end_inset

 subsets (folds).
 
\end_layout

\begin_layout Itemize
For a set of potential 
\begin_inset Formula $\lambda$
\end_inset

s estimate the algorithm on 
\begin_inset Formula $f-1$
\end_inset

 of the folds and then see which 
\begin_inset Formula $\lambda$
\end_inset

 predicts best on the 
\begin_inset Formula $f$
\end_inset

th fold (the hold-out fold).
 This procedure is called (
\begin_inset Formula $f$
\end_inset

-fold)
\emph on
 cross-validation
\end_layout

\begin_layout Itemize
This is cool because they expand the set of predictors we can consider:
\end_layout

\begin_deeper
\begin_layout Itemize
They allow for 
\emph on
wide
\emph default
 data (e.g.
 text data, internet activity data) where we have many more variables than
 data points.
\end_layout

\begin_layout Itemize
Much more flexible functional forms
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
But NB, you will get a great 
\begin_inset Formula $\hat{y}$
\end_inset

, you don't get any guarantee that you have useful 
\begin_inset Formula $\hat{\beta}$
\end_inset

s
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kleinberg et al 2015: Policy Examples
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Who should get joint replacement surgery?
\end_layout

\begin_layout Enumerate
Which teacher will have the greatest value added?
\end_layout

\begin_layout Enumerate
How long will unemployment spells last?
\end_layout

\begin_layout Enumerate
How should health inspections be targeted?
\end_layout

\begin_layout Enumerate
Predicting at-risk youth
\end_layout

\begin_layout Enumerate
Which borrowers are credit-worthy?
\end_layout

\end_deeper
\begin_layout Subsection
Kleinberg, Lakkarju, Leskovec, Ludwig & Mullainathan (WP 2017) 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
protect
\backslash
textit{
\end_layout

\end_inset

Human Decisions and Machine Predictions
\begin_inset ERT
status open

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kleinberg et al 2017: Introduction
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Machine learning is all about prediction.
 This paper looks at what, at first glance, is an ideal application.
\end_layout

\begin_layout Itemize
After an arrest, judges decide whether defendants await trial in jail or
 at home.
 Law says this bail decision depends only on the judges prediction of whether
 the defendant wil reoffend or flee if released.
\end_layout

\begin_layout Itemize
Here use data on 758,027 defendands in NYC between 2008 and 2013 to predict
 probability of reoffending.
\end_layout

\begin_layout Itemize

\emph on
Can we use these predictions to understand and improve judges' decisions?
\end_layout

\begin_layout Enumerate
Needs methods from 
\emph on
both
\emph default
 machine learning and microeconometrics.
\end_layout

\begin_layout Enumerate

\emph on
Omitted payoffs:
\emph default
 We can predict probability of reoffending, but what if there are other
 things judges care about?
\end_layout

\begin_layout Enumerate

\emph on
Selective labels:
\emph default
 We only see the crime outcomes of people who are released.
 What would those who were jailed have done if they had been released?
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kleinberg et al 2017: Data & Context
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Shortly after someone is arrested, there's a bail hearing.
 Judges can a) release; b) set a dollar bail; c) detain with no chance of
 bail.
\end_layout

\begin_layout Itemize
Judges asked to decide based on a prediction of whether the defendant would
 fail to appear in court or be re-arrested for a new crime.
\end_layout

\begin_layout Itemize
Data on all arrests in NYC between 11/2008 and 11/2013: 1,460,462 cases.
\end_layout

\begin_layout Itemize
758,027 subject to a pre-trial release decision.
 Randomly sample 203,338 cases to keep in a 
\begin_inset Quotes eld
\end_inset

lock box
\begin_inset Quotes erd
\end_inset

: Not used for training the algorithm or writing drafts of the paper, will
 only be used for the final version.
\end_layout

\begin_layout Itemize
Working data is 554,689 cases.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename KleinbergEtAl2017_1.png
	lyxscale 50
	width 100text%

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename KleinbergEtAl2017_2.png
	lyxscale 50
	width 100text%

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename KleinbergEtAl2017_3.png
	lyxscale 50
	width 100text%

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kleinberg et al 2017: Machine Learning
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Form predictions 
\begin_inset Formula $\hat{y}=m\left(x\right)$
\end_inset

 to minimize loss function 
\begin_inset Formula $L\left(y,\hat{y}\right)$
\end_inset


\end_layout

\begin_layout Itemize
Consider functions 
\begin_inset Formula $m\left(x\right)$
\end_inset

 that generate predicted probabilities in 
\begin_inset Formula $\left[0,1\right]$
\end_inset


\end_layout

\begin_layout Itemize
Use Bernoulli loss
\begin_inset Formula 
\[
L\left(y_{i},m\left(x_{i}\right)\right)=-\left[y_{i}\log\left(m\left(x_{i}\right)\right)+\left(1-y_{i}\right)\log\left(1-m\left(x_{i}\right)\right)\right]
\]

\end_inset


\end_layout

\begin_layout Itemize
For this paper, use gradient boosted decision trees to form 
\begin_inset Formula $m\left(x_{i}\right)$
\end_inset

.
 Average multiple trees built sequentially where each iteration up-weights
 observations that fit poorly in the sequence of trees up to that point.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename KleinbergEtAl2017_4.png
	lyxscale 50
	height 100theight%

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kleinberg et al 2017: Misranking?
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
The riskiest 1% of defendants have a predicted risk of 62.6%, but 48.5% of
 them are released, and reoffend 56.3% of the time!
\end_layout

\begin_layout Itemize
Suggests Judges are misranking the defendants.
 But could also be that the judges use an even higher threshold risk for
 detention.
\end_layout

\begin_layout Itemize
Look across judges of different leniencies.
 If it's the case that more lenient judges just have a higher threshold
 risk, then the predicted risk scores should be good at predicting which
 people will be jailed by a less lenient judge relative to a lenient judge.
\end_layout

\begin_layout Itemize
Identified by quasi-random assignment of cases to judges: It depends on
 who happens to be on duty in that bourough 
\begin_inset Formula $\times$
\end_inset

 court house 
\begin_inset Formula $\times$
\end_inset

 day
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename KleinbergEtAl2017_5.png
	width 100text%

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename KleinbergEtAl2017_6.png
	height 100theight%

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kleinberg et al 2017: Alternative Policies
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
So can we use the algorithm to improve release decisions?
\end_layout

\begin_layout Itemize
Consider 2 types of policies
\end_layout

\begin_layout Enumerate
Contraction: A warning whenever the judge is about to release a high-risk
 defendant.
 Like a driver assist system that warns when the car does something potentially
 dangerous.
\end_layout

\begin_layout Enumerate
Reranking: Risk tool danks defendants and makes recommendations for all
 decisions.
 Like an auto-pilot that the judge could overrule.
 Improves both high- and low-risk decisions.
\end_layout

\begin_layout Itemize
Now we need a counterfactual: 2 issues arise
\end_layout

\begin_layout Enumerate
Compliance: Will the judge pay attention to the warnings / comply with the
 risk score?
\end_layout

\begin_layout Enumerate
Reranking involves releasing some of the jailed defendants.
 How much crime 
\emph on
would they have
\emph default
 committed? A missing labels problem.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kleinberg et al 2017: Contraction
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
What threshold risk score should trigger the warning? 
\end_layout

\begin_layout Itemize
Low threshold averts more crime, but triggers more warnings, and increases
 jailings of people who would not reoffend.
 
\end_layout

\begin_layout Itemize
Implicitly, the judges have some threshold too that we can compare to.
 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename KleinbergEtAl2017_7.png
	height 100theight%

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kleinberg et al 2017: Reranking
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Reranking creates a selective labels problem: We only see crime rates of
 those who are jailed.
 
\end_layout

\begin_layout Itemize
Approach here: Impute based on observables.
 Then do 2 bounding exercises
\end_layout

\begin_deeper
\begin_layout Enumerate
Decompose algorithm's gains into 
\end_layout

\begin_deeper
\begin_layout Enumerate
jail a high risk defendant and release an 
\emph on
average 
\emph default
risk defendant
\end_layout

\begin_layout Enumerate
release a low risk defendant and jail an 
\emph on
average
\emph default
 risk defendant.
 Selective labels problem only comes in here.
\end_layout

\end_deeper
\begin_layout Enumerate
Assume imputing underestimates by 
\begin_inset Formula $\alpha$
\end_inset

 and use a prediced risk of 
\begin_inset Formula $\min\left\{ 1,\alpha\hat{y}\right\} $
\end_inset

 and do robustness to 
\begin_inset Formula $\alpha$
\end_inset

.
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename KleinbergEtAl2017_8.png
	height 100theight%

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename KleinbergEtAl2017_9.png
	height 100theight%

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename KleinbergEtAl2017_10.png
	width 105text%

\end_inset


\end_layout

\end_deeper
\begin_layout Section
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Data & Technology in Development
\end_layout

\end_inset

Examples of Data & Technology in Development
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Papers
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Abelson targeting
\end_layout

\begin_layout Itemize
Blumenstock et al phones prediction
\end_layout

\begin_layout Itemize
Marshall's satellite prediction
\end_layout

\begin_layout Itemize
Banerjee et al ID cards
\end_layout

\begin_layout Itemize
Muralidharan et al
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Papers
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Abelson targeting
\end_layout

\begin_layout Itemize
Blumenstock et al phones prediction
\end_layout

\begin_layout Itemize
Marshall's satellite prediction
\end_layout

\begin_layout Itemize
Banerjee et al ID cards
\end_layout

\begin_layout Itemize
Muralidharan et al
\end_layout

\end_deeper
\end_body
\end_document
